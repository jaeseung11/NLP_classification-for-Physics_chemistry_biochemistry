{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "65a135a6",
      "metadata": {
        "papermill": {
          "duration": 0.025891,
          "end_time": "2022-09-26T11:20:13.028773",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.002882",
          "status": "completed"
        },
        "tags": [],
        "id": "65a135a6"
      },
      "source": [
        "# Introduction to NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69eeed54",
      "metadata": {
        "papermill": {
          "duration": 0.012098,
          "end_time": "2022-09-26T11:20:13.053495",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.041397",
          "status": "completed"
        },
        "tags": [],
        "id": "69eeed54"
      },
      "source": [
        "**NLP stands for Natural Language Processing, which is a part of Computer Science, Human language, and Artificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages. It helps developers to organize knowledge for performing tasks such as translation, automatic summarization, Named Entity Recognition (NER), speech recognition, relationship extraction, and topic segmentation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acde129e",
      "metadata": {
        "papermill": {
          "duration": 0.011952,
          "end_time": "2022-09-26T11:20:13.077705",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.065753",
          "status": "completed"
        },
        "tags": [],
        "id": "acde129e"
      },
      "source": [
        "![](https://image.shutterstock.com/image-photo/cognitive-computing-concept-future-technology-260nw-1259313322.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52d256c6",
      "metadata": {
        "papermill": {
          "duration": 0.016651,
          "end_time": "2022-09-26T11:20:13.106541",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.089890",
          "status": "completed"
        },
        "tags": [],
        "id": "52d256c6"
      },
      "source": [
        "# Components of NLP\n",
        "There are the following two components of NLP -\n",
        "\n",
        " **1. Natural Language Understanding (NLU)**\n",
        "\n",
        "Natural Language Understanding (NLU) helps the machine to understand and analyse human language by extracting the metadata from content such as concepts, entities, keywords, emotion, relations, and semantic roles.\n",
        "\n",
        "NLU mainly used in Business applications to understand the customer's problem in both spoken and written language.\n",
        "\n",
        "NLU involves the following tasks -\n",
        "\n",
        "It is used to map the given input into useful representation.\n",
        "It is used to analyze different aspects of the language.\n",
        "\n",
        " **2. Natural Language Generation (NLG)**\n",
        "\n",
        "Natural Language Generation (NLG) acts as a translator that converts the computerized data into natural language representation. It mainly involves Text planning, Sentence planning, and Text Realization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d63fcda",
      "metadata": {
        "papermill": {
          "duration": 0.02053,
          "end_time": "2022-09-26T11:20:13.149284",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.128754",
          "status": "completed"
        },
        "tags": [],
        "id": "4d63fcda"
      },
      "source": [
        "# Table of contents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca09b1c5",
      "metadata": {
        "papermill": {
          "duration": 0.017942,
          "end_time": "2022-09-26T11:20:13.184454",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.166512",
          "status": "completed"
        },
        "tags": [],
        "id": "ca09b1c5"
      },
      "source": [
        "* Text processing\n",
        "* Tokenization \n",
        "*  Normalization\n",
        "*       Lemmatization & stemming\n",
        "        Removing Stopwords\n",
        "* Text representation\n",
        "* Bag Of Words\n",
        "* N-Grams\n",
        "* TF-IDF (Term Frequency and Inverse Document Frequency)\n",
        "*  Modelling : Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cceda924",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.217013Z",
          "iopub.status.busy": "2022-09-26T11:20:13.216374Z",
          "iopub.status.idle": "2022-09-26T11:20:13.227783Z",
          "shell.execute_reply": "2022-09-26T11:20:13.226920Z"
        },
        "papermill": {
          "duration": 0.027368,
          "end_time": "2022-09-26T11:20:13.229975",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.202607",
          "status": "completed"
        },
        "tags": [],
        "id": "cceda924"
      },
      "outputs": [],
      "source": [
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2417c8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.256197Z",
          "iopub.status.busy": "2022-09-26T11:20:13.255941Z",
          "iopub.status.idle": "2022-09-26T11:20:13.345564Z",
          "shell.execute_reply": "2022-09-26T11:20:13.344615Z"
        },
        "papermill": {
          "duration": 0.105912,
          "end_time": "2022-09-26T11:20:13.348564",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.242652",
          "status": "completed"
        },
        "tags": [],
        "id": "5e2417c8"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"../input/physics-vs-chemistry-vs-biology/dataset/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e497fc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.382334Z",
          "iopub.status.busy": "2022-09-26T11:20:13.381892Z",
          "iopub.status.idle": "2022-09-26T11:20:13.403200Z",
          "shell.execute_reply": "2022-09-26T11:20:13.402208Z"
        },
        "papermill": {
          "duration": 0.03772,
          "end_time": "2022-09-26T11:20:13.405473",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.367753",
          "status": "completed"
        },
        "tags": [],
        "id": "69e497fc",
        "outputId": "b4d27d2c-40c5-4336-ec51-df1b137a68fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0x840</td>\n",
              "      <td>A few things. You might have negative- frequen...</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0xbf0</td>\n",
              "      <td>Is it so hard to believe that there exist part...</td>\n",
              "      <td>Physics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0x1dfc</td>\n",
              "      <td>There are bees</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0xc7e</td>\n",
              "      <td>I'm a medication technician. And that's alot o...</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0xbba</td>\n",
              "      <td>Cesium is such a pretty metal.</td>\n",
              "      <td>Chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>0x1e02</td>\n",
              "      <td>I make similar observations over the last week...</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>0xc8d</td>\n",
              "      <td>You would know.</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>0x723</td>\n",
              "      <td>Also use the correct number of sig figs</td>\n",
              "      <td>Chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8693</th>\n",
              "      <td>0x667</td>\n",
              "      <td>What about the ethical delimmas,  groundbreaki...</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8694</th>\n",
              "      <td>0x1476</td>\n",
              "      <td>I would like to know too.</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8695 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Id                                            Comment      Topic\n",
              "0      0x840  A few things. You might have negative- frequen...    Biology\n",
              "1      0xbf0  Is it so hard to believe that there exist part...    Physics\n",
              "2     0x1dfc                                     There are bees    Biology\n",
              "3      0xc7e  I'm a medication technician. And that's alot o...    Biology\n",
              "4      0xbba                     Cesium is such a pretty metal.  Chemistry\n",
              "...      ...                                                ...        ...\n",
              "8690  0x1e02  I make similar observations over the last week...    Biology\n",
              "8691   0xc8d                                    You would know.    Biology\n",
              "8692   0x723            Also use the correct number of sig figs  Chemistry\n",
              "8693   0x667  What about the ethical delimmas,  groundbreaki...    Biology\n",
              "8694  0x1476                          I would like to know too.    Biology\n",
              "\n",
              "[8695 rows x 3 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad43ed42",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.433631Z",
          "iopub.status.busy": "2022-09-26T11:20:13.432716Z",
          "iopub.status.idle": "2022-09-26T11:20:13.442898Z",
          "shell.execute_reply": "2022-09-26T11:20:13.442058Z"
        },
        "papermill": {
          "duration": 0.026125,
          "end_time": "2022-09-26T11:20:13.444856",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.418731",
          "status": "completed"
        },
        "tags": [],
        "id": "ad43ed42"
      },
      "outputs": [],
      "source": [
        "data.drop('Id',axis=1 , inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79953095",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.471685Z",
          "iopub.status.busy": "2022-09-26T11:20:13.470907Z",
          "iopub.status.idle": "2022-09-26T11:20:13.481448Z",
          "shell.execute_reply": "2022-09-26T11:20:13.480491Z"
        },
        "papermill": {
          "duration": 0.025737,
          "end_time": "2022-09-26T11:20:13.483349",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.457612",
          "status": "completed"
        },
        "tags": [],
        "id": "79953095",
        "outputId": "e4481ddc-7ec6-440b-c1da-b16c707e7a8c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "      <th>Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A few things. You might have negative- frequen...</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is it so hard to believe that there exist part...</td>\n",
              "      <td>Physics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>There are bees</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I'm a medication technician. And that's alot o...</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cesium is such a pretty metal.</td>\n",
              "      <td>Chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>I make similar observations over the last week...</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>You would know.</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>Also use the correct number of sig figs</td>\n",
              "      <td>Chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8693</th>\n",
              "      <td>What about the ethical delimmas,  groundbreaki...</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8694</th>\n",
              "      <td>I would like to know too.</td>\n",
              "      <td>Biology</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8695 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Comment      Topic\n",
              "0     A few things. You might have negative- frequen...    Biology\n",
              "1     Is it so hard to believe that there exist part...    Physics\n",
              "2                                        There are bees    Biology\n",
              "3     I'm a medication technician. And that's alot o...    Biology\n",
              "4                        Cesium is such a pretty metal.  Chemistry\n",
              "...                                                 ...        ...\n",
              "8690  I make similar observations over the last week...    Biology\n",
              "8691                                    You would know.    Biology\n",
              "8692            Also use the correct number of sig figs  Chemistry\n",
              "8693  What about the ethical delimmas,  groundbreaki...    Biology\n",
              "8694                          I would like to know too.    Biology\n",
              "\n",
              "[8695 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013a2a7e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.510446Z",
          "iopub.status.busy": "2022-09-26T11:20:13.509635Z",
          "iopub.status.idle": "2022-09-26T11:20:13.515602Z",
          "shell.execute_reply": "2022-09-26T11:20:13.514649Z"
        },
        "papermill": {
          "duration": 0.02142,
          "end_time": "2022-09-26T11:20:13.517517",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.496097",
          "status": "completed"
        },
        "tags": [],
        "id": "013a2a7e",
        "outputId": "5b0a4dee-2c0c-4b32-c8e0-dfe434cf6a7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8695, 2)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a13217",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.544665Z",
          "iopub.status.busy": "2022-09-26T11:20:13.543888Z",
          "iopub.status.idle": "2022-09-26T11:20:13.560984Z",
          "shell.execute_reply": "2022-09-26T11:20:13.559367Z"
        },
        "papermill": {
          "duration": 0.032923,
          "end_time": "2022-09-26T11:20:13.563146",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.530223",
          "status": "completed"
        },
        "tags": [],
        "id": "c5a13217",
        "outputId": "c9cb2272-fe81-4180-a239-a000a03d435f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8695 entries, 0 to 8694\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Comment  8695 non-null   object\n",
            " 1   Topic    8695 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 136.0+ KB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed199926",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.591250Z",
          "iopub.status.busy": "2022-09-26T11:20:13.590969Z",
          "iopub.status.idle": "2022-09-26T11:20:13.595890Z",
          "shell.execute_reply": "2022-09-26T11:20:13.594912Z"
        },
        "papermill": {
          "duration": 0.021106,
          "end_time": "2022-09-26T11:20:13.597683",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.576577",
          "status": "completed"
        },
        "tags": [],
        "id": "ed199926"
      },
      "outputs": [],
      "source": [
        "data.drop('Topic',axis=1)\n",
        "y=data['Topic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff75366d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.625016Z",
          "iopub.status.busy": "2022-09-26T11:20:13.624170Z",
          "iopub.status.idle": "2022-09-26T11:20:13.631422Z",
          "shell.execute_reply": "2022-09-26T11:20:13.630478Z"
        },
        "papermill": {
          "duration": 0.022744,
          "end_time": "2022-09-26T11:20:13.633356",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.610612",
          "status": "completed"
        },
        "tags": [],
        "id": "ff75366d",
        "outputId": "2d07619c-d342-4f0d-c83f-f53004a4b87a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         Biology\n",
              "1         Physics\n",
              "2         Biology\n",
              "3         Biology\n",
              "4       Chemistry\n",
              "          ...    \n",
              "8690      Biology\n",
              "8691      Biology\n",
              "8692    Chemistry\n",
              "8693      Biology\n",
              "8694      Biology\n",
              "Name: Topic, Length: 8695, dtype: object"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a221133",
      "metadata": {
        "papermill": {
          "duration": 0.012526,
          "end_time": "2022-09-26T11:20:13.658742",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.646216",
          "status": "completed"
        },
        "tags": [],
        "id": "1a221133"
      },
      "source": [
        "# Text processing "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a3b989",
      "metadata": {
        "papermill": {
          "duration": 0.012497,
          "end_time": "2022-09-26T11:20:13.683988",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.671491",
          "status": "completed"
        },
        "tags": [],
        "id": "b5a3b989"
      },
      "source": [
        "> Text processing contains two main phases, which are tokenization and normalization .\n",
        "\n",
        "> **Tokenization** is the process of splitting a longer string of text into smaller pieces, or tokens. \n",
        "\n",
        "> **Normalization** referring to convert number to their word equivalent, remove punctuation, convert all text to the same case, remove stopwords, remove noise, lemmatizing and stemming.\n",
        "> \n",
        "> * Stemming — removing affixes (suffixed, prefixes, infixes, circumfixes), For example, running to run\n",
        "> \n",
        "> * Lemmatization — capture canonical form based on a word’s lemma. For example, better to good"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e256041",
      "metadata": {
        "papermill": {
          "duration": 0.012585,
          "end_time": "2022-09-26T11:20:13.710129",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.697544",
          "status": "completed"
        },
        "tags": [],
        "id": "0e256041"
      },
      "source": [
        "# 1. Tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09aa5734",
      "metadata": {
        "papermill": {
          "duration": 0.012499,
          "end_time": "2022-09-26T11:20:13.735394",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.722895",
          "status": "completed"
        },
        "tags": [],
        "id": "09aa5734"
      },
      "source": [
        "> Using tokenizer to separate the sentences into a list of single words (tokens)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ba0f11",
      "metadata": {
        "papermill": {
          "duration": 0.012596,
          "end_time": "2022-09-26T11:20:13.760722",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.748126",
          "status": "completed"
        },
        "tags": [],
        "id": "49ba0f11"
      },
      "source": [
        "![](https://miro.medium.com/max/1050/0*EKgminT7W-0R4Iae.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e23fee2",
      "metadata": {
        "papermill": {
          "duration": 0.012661,
          "end_time": "2022-09-26T11:20:13.786082",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.773421",
          "status": "completed"
        },
        "tags": [],
        "id": "7e23fee2"
      },
      "source": [
        "> There are several tokenizer modules in NLTK libraries  WordPunctTokenizer used above. For example, word_tokenize and RegexpTokenizer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4736406c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.813612Z",
          "iopub.status.busy": "2022-09-26T11:20:13.813255Z",
          "iopub.status.idle": "2022-09-26T11:20:13.817603Z",
          "shell.execute_reply": "2022-09-26T11:20:13.816674Z"
        },
        "papermill": {
          "duration": 0.020718,
          "end_time": "2022-09-26T11:20:13.819492",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.798774",
          "status": "completed"
        },
        "tags": [],
        "id": "4736406c"
      },
      "outputs": [],
      "source": [
        "data1=data['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9c5cf6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.847677Z",
          "iopub.status.busy": "2022-09-26T11:20:13.846886Z",
          "iopub.status.idle": "2022-09-26T11:20:13.854567Z",
          "shell.execute_reply": "2022-09-26T11:20:13.853654Z"
        },
        "papermill": {
          "duration": 0.023319,
          "end_time": "2022-09-26T11:20:13.856508",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.833189",
          "status": "completed"
        },
        "tags": [],
        "id": "fb9c5cf6",
        "outputId": "10e54bcf-5fc3-4d75-f4fa-6e2200b5210d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       A few things. You might have negative- frequen...\n",
              "1       Is it so hard to believe that there exist part...\n",
              "2                                          There are bees\n",
              "3       I'm a medication technician. And that's alot o...\n",
              "4                          Cesium is such a pretty metal.\n",
              "                              ...                        \n",
              "8690    I make similar observations over the last week...\n",
              "8691                                      You would know.\n",
              "8692              Also use the correct number of sig figs\n",
              "8693    What about the ethical delimmas,  groundbreaki...\n",
              "8694                            I would like to know too.\n",
              "Name: Comment, Length: 8695, dtype: object"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c061ea74",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.885592Z",
          "iopub.status.busy": "2022-09-26T11:20:13.884156Z",
          "iopub.status.idle": "2022-09-26T11:20:13.890801Z",
          "shell.execute_reply": "2022-09-26T11:20:13.889777Z"
        },
        "papermill": {
          "duration": 0.023178,
          "end_time": "2022-09-26T11:20:13.892821",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.869643",
          "status": "completed"
        },
        "tags": [],
        "id": "c061ea74",
        "outputId": "0d46ebe3-8a3f-4979-c970-d781136af20a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8695,)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc0474f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.922852Z",
          "iopub.status.busy": "2022-09-26T11:20:13.921855Z",
          "iopub.status.idle": "2022-09-26T11:20:13.928352Z",
          "shell.execute_reply": "2022-09-26T11:20:13.927380Z"
        },
        "papermill": {
          "duration": 0.023745,
          "end_time": "2022-09-26T11:20:13.930375",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.906630",
          "status": "completed"
        },
        "tags": [],
        "id": "9dc0474f",
        "outputId": "7523c87c-d958-44e1-bd36-3a37db368ca6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4303852c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:13.959818Z",
          "iopub.status.busy": "2022-09-26T11:20:13.958722Z",
          "iopub.status.idle": "2022-09-26T11:20:15.288351Z",
          "shell.execute_reply": "2022-09-26T11:20:15.287352Z"
        },
        "papermill": {
          "duration": 1.347515,
          "end_time": "2022-09-26T11:20:15.291413",
          "exception": false,
          "start_time": "2022-09-26T11:20:13.943898",
          "status": "completed"
        },
        "tags": [],
        "id": "4303852c"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4635b91a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:15.333623Z",
          "iopub.status.busy": "2022-09-26T11:20:15.333073Z",
          "iopub.status.idle": "2022-09-26T11:20:15.338284Z",
          "shell.execute_reply": "2022-09-26T11:20:15.337284Z"
        },
        "papermill": {
          "duration": 0.030144,
          "end_time": "2022-09-26T11:20:15.342403",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.312259",
          "status": "completed"
        },
        "tags": [],
        "id": "4635b91a"
      },
      "outputs": [],
      "source": [
        "WPT=WordPunctTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e85ac07",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:15.382714Z",
          "iopub.status.busy": "2022-09-26T11:20:15.382178Z",
          "iopub.status.idle": "2022-09-26T11:20:15.408004Z",
          "shell.execute_reply": "2022-09-26T11:20:15.407109Z"
        },
        "papermill": {
          "duration": 0.048301,
          "end_time": "2022-09-26T11:20:15.410106",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.361805",
          "status": "completed"
        },
        "tags": [],
        "id": "9e85ac07"
      },
      "outputs": [],
      "source": [
        "word_punct_token = WPT.tokenize(data1[0])\n",
        "Word_tokenize= word_tokenize(data1[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "864d26b2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:15.450839Z",
          "iopub.status.busy": "2022-09-26T11:20:15.450468Z",
          "iopub.status.idle": "2022-09-26T11:20:15.455850Z",
          "shell.execute_reply": "2022-09-26T11:20:15.454924Z"
        },
        "papermill": {
          "duration": 0.033487,
          "end_time": "2022-09-26T11:20:15.461901",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.428414",
          "status": "completed"
        },
        "tags": [],
        "id": "864d26b2",
        "outputId": "df75aa35-5f46-4822-d26a-9ab863872189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A', 'few', 'things', '.', 'You', 'might', 'have', 'negative-', 'frequency', 'dependent', 'selection', 'going', 'on', 'where', 'the', 'least', 'common', 'phenotype', ',', 'reflected', 'by', 'genotype', ',', 'is', 'going', 'to', 'have', 'an', 'advantage', 'in', 'the', 'environment', '.', 'For', 'instance', ',', 'if', 'a', 'prey', 'animal', 'such', 'as', 'a', 'vole', 'were', 'to', 'have', 'a', 'light', 'and', 'a', 'dark', 'phenotype', ',', 'a', 'predator', 'might', 'recognize', 'the', 'more', 'common', 'phenotype', 'as', 'food', '.', 'So', 'if', 'the', 'light', 'voles', 'are', 'more', 'common', ',', 'foxes', 'may', 'be', 'keeping', 'a', 'closer', 'eye', 'out', 'for', 'light', 'phenotypic', 'voles', ',', 'recognising', 'them', 'as', 'good', 'prey', '.', 'This', 'would', 'reduce', 'the', 'light', 'causing', 'alleles', 'due', 'to', 'increased', 'predation', 'and', 'the', 'dark', 'genotypes', 'would', 'increase', 'their', 'proportion', 'of', 'the', 'population', 'until', 'this', 'scenario', 'is', 'reversed', '.', 'This', 'cycle', 'continues', 'perpetually', '.', '\\\\n\\\\nHowever', ',', 'this', 'is', 'unlikely', 'to', 'be', 'strictly', 'yearly', 'as', 'it', 'usually', 'takes', 'more', 'time', 'than', 'a', 'year', 'for', 'an', 'entire', 'populations', 'allele', 'frequencies', 'to', 'change', 'enough', 'to', 'make', 'a', 'large', 'enough', 'difference', 'to', 'alter', 'fitness', '.', '\\\\n\\\\nMore', 'likely', 'on', 'a', '*', 'year', 'to', 'year', '*', 'basis', ',', 'the', 'population', 'is', 'experiencing', 'fluctuating', 'selection', 'where', 'alternating', 'conditions', 'in', 'the', 'environment', 'favor', 'one', 'genotype', 'over', 'another', '.', 'Perhaps', 'a', 'plant', 'species', 'is', 'living', 'in', 'an', 'area', 'that', 'is', 'flooded', 'every', 'other', 'year', 'and', 'the', 'two', 'phenotypes', 'in', 'the', 'population', 'are', 'plants', 'that', 'do', 'much', 'better', 'in', 'the', 'dryer', 'year', 'and', 'one', 'that', 'does', 'better', 'in', 'the', 'wet', 'year', '.', 'If', 'there', 'is', 'no', 'flooding', ',', 'the', 'dry-type', 'genotype', 'will', 'have', 'more', 'fitness', 'leading', 'to', 'more', 'offspring', 'and', 'therefore', 'more', 'dry', 'alleles', 'in', 'the', 'population', ',', 'however', ',', 'in', 'flooded', 'years', 'the', 'wet-liking', 'phenotype', 'will', 'do', 'better', 'and', 'propagate', 'the', 'wet', 'genes', '.']\n"
          ]
        }
      ],
      "source": [
        "print(Word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb246f18",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:15.516426Z",
          "iopub.status.busy": "2022-09-26T11:20:15.515721Z",
          "iopub.status.idle": "2022-09-26T11:20:15.521115Z",
          "shell.execute_reply": "2022-09-26T11:20:15.520145Z"
        },
        "papermill": {
          "duration": 0.038209,
          "end_time": "2022-09-26T11:20:15.523814",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.485605",
          "status": "completed"
        },
        "tags": [],
        "id": "eb246f18",
        "outputId": "206670ba-4c97-4f28-e021-760db2738451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A', 'few', 'things', '.', 'You', 'might', 'have', 'negative', '-', 'frequency', 'dependent', 'selection', 'going', 'on', 'where', 'the', 'least', 'common', 'phenotype', ',', 'reflected', 'by', 'genotype', ',', 'is', 'going', 'to', 'have', 'an', 'advantage', 'in', 'the', 'environment', '.', 'For', 'instance', ',', 'if', 'a', 'prey', 'animal', 'such', 'as', 'a', 'vole', 'were', 'to', 'have', 'a', 'light', 'and', 'a', 'dark', 'phenotype', ',', 'a', 'predator', 'might', 'recognize', 'the', 'more', 'common', 'phenotype', 'as', 'food', '.', 'So', 'if', 'the', 'light', 'voles', 'are', 'more', 'common', ',', 'foxes', 'may', 'be', 'keeping', 'a', 'closer', 'eye', 'out', 'for', 'light', 'phenotypic', 'voles', ',', 'recognising', 'them', 'as', 'good', 'prey', '.', 'This', 'would', 'reduce', 'the', 'light', 'causing', 'alleles', 'due', 'to', 'increased', 'predation', 'and', 'the', 'dark', 'genotypes', 'would', 'increase', 'their', 'proportion', 'of', 'the', 'population', 'until', 'this', 'scenario', 'is', 'reversed', '.', 'This', 'cycle', 'continues', 'perpetually', '.', '\\\\', 'n', '\\\\', 'nHowever', ',', 'this', 'is', 'unlikely', 'to', 'be', 'strictly', 'yearly', 'as', 'it', 'usually', 'takes', 'more', 'time', 'than', 'a', 'year', 'for', 'an', 'entire', 'populations', 'allele', 'frequencies', 'to', 'change', 'enough', 'to', 'make', 'a', 'large', 'enough', 'difference', 'to', 'alter', 'fitness', '.', '\\\\', 'n', '\\\\', 'nMore', 'likely', 'on', 'a', '*', 'year', 'to', 'year', '*', 'basis', ',', 'the', 'population', 'is', 'experiencing', 'fluctuating', 'selection', 'where', 'alternating', 'conditions', 'in', 'the', 'environment', 'favor', 'one', 'genotype', 'over', 'another', '.', 'Perhaps', 'a', 'plant', 'species', 'is', 'living', 'in', 'an', 'area', 'that', 'is', 'flooded', 'every', 'other', 'year', 'and', 'the', 'two', 'phenotypes', 'in', 'the', 'population', 'are', 'plants', 'that', 'do', 'much', 'better', 'in', 'the', 'dryer', 'year', 'and', 'one', 'that', 'does', 'better', 'in', 'the', 'wet', 'year', '.', 'If', 'there', 'is', 'no', 'flooding', ',', 'the', 'dry', '-', 'type', 'genotype', 'will', 'have', 'more', 'fitness', 'leading', 'to', 'more', 'offspring', 'and', 'therefore', 'more', 'dry', 'alleles', 'in', 'the', 'population', ',', 'however', ',', 'in', 'flooded', 'years', 'the', 'wet', '-', 'liking', 'phenotype', 'will', 'do', 'better', 'and', 'propagate', 'the', 'wet', 'genes', '.']\n"
          ]
        }
      ],
      "source": [
        "print(word_punct_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fd1b9b4",
      "metadata": {
        "papermill": {
          "duration": 0.01317,
          "end_time": "2022-09-26T11:20:15.552076",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.538906",
          "status": "completed"
        },
        "tags": [],
        "id": "4fd1b9b4"
      },
      "source": [
        "# 2. Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e2c316",
      "metadata": {
        "papermill": {
          "duration": 0.01307,
          "end_time": "2022-09-26T11:20:15.578370",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.565300",
          "status": "completed"
        },
        "tags": [],
        "id": "b2e2c316"
      },
      "source": [
        "**Removing Stopwords**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4a14de",
      "metadata": {
        "papermill": {
          "duration": 0.013073,
          "end_time": "2022-09-26T11:20:15.604746",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.591673",
          "status": "completed"
        },
        "tags": [],
        "id": "2b4a14de"
      },
      "source": [
        "> Stopwords referring to the word which does not carry much insight, such as preposition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8925c3e8",
      "metadata": {
        "papermill": {
          "duration": 0.013023,
          "end_time": "2022-09-26T11:20:15.631001",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.617978",
          "status": "completed"
        },
        "tags": [],
        "id": "8925c3e8"
      },
      "source": [
        "![](https://user.oc-static.com/upload/2021/01/06/16099626487943_P1C2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f162d690",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:15.659444Z",
          "iopub.status.busy": "2022-09-26T11:20:15.659054Z",
          "iopub.status.idle": "2022-09-26T11:20:15.663580Z",
          "shell.execute_reply": "2022-09-26T11:20:15.662493Z"
        },
        "papermill": {
          "duration": 0.02176,
          "end_time": "2022-09-26T11:20:15.665961",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.644201",
          "status": "completed"
        },
        "tags": [],
        "id": "f162d690"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0145c9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:15.693664Z",
          "iopub.status.busy": "2022-09-26T11:20:15.693374Z",
          "iopub.status.idle": "2022-09-26T11:20:15.703794Z",
          "shell.execute_reply": "2022-09-26T11:20:15.702519Z"
        },
        "papermill": {
          "duration": 0.027218,
          "end_time": "2022-09-26T11:20:15.706409",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.679191",
          "status": "completed"
        },
        "tags": [],
        "id": "9d0145c9",
        "outputId": "75e67da7-b923-437d-cdd2-08c1ae125814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "stop_words = stopwords.words('english')# Get the list of stop words\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd27018b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:15.737199Z",
          "iopub.status.busy": "2022-09-26T11:20:15.735419Z",
          "iopub.status.idle": "2022-09-26T11:20:15.742264Z",
          "shell.execute_reply": "2022-09-26T11:20:15.741201Z"
        },
        "papermill": {
          "duration": 0.02456,
          "end_time": "2022-09-26T11:20:15.745403",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.720843",
          "status": "completed"
        },
        "tags": [],
        "id": "dd27018b",
        "outputId": "32f5450b-fd57-487c-9bd5-03063a6cd807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A', 'things', '.', 'You', 'might', 'negative', '-', 'frequency', 'dependent', 'selection', 'going', 'least', 'common', 'phenotype', ',', 'reflected', 'genotype', ',', 'going', 'advantage', 'environment', '.', 'For', 'instance', ',', 'prey', 'animal', 'vole', 'light', 'dark', 'phenotype', ',', 'predator', 'might', 'recognize', 'common', 'phenotype', 'food', '.', 'So', 'light', 'voles', 'common', ',', 'foxes', 'may', 'keeping', 'closer', 'eye', 'light', 'phenotypic', 'voles', ',', 'recognising', 'good', 'prey', '.', 'This', 'would', 'reduce', 'light', 'causing', 'alleles', 'due', 'increased', 'predation', 'dark', 'genotypes', 'would', 'increase', 'proportion', 'population', 'scenario', 'reversed', '.', 'This', 'cycle', 'continues', 'perpetually', '.', '\\\\', 'n', '\\\\', 'nHowever', ',', 'unlikely', 'strictly', 'yearly', 'usually', 'takes', 'time', 'year', 'entire', 'populations', 'allele', 'frequencies', 'change', 'enough', 'make', 'large', 'enough', 'difference', 'alter', 'fitness', '.', '\\\\', 'n', '\\\\', 'nMore', 'likely', '*', 'year', 'year', '*', 'basis', ',', 'population', 'experiencing', 'fluctuating', 'selection', 'alternating', 'conditions', 'environment', 'favor', 'one', 'genotype', 'another', '.', 'Perhaps', 'plant', 'species', 'living', 'area', 'flooded', 'every', 'year', 'two', 'phenotypes', 'population', 'plants', 'much', 'better', 'dryer', 'year', 'one', 'better', 'wet', 'year', '.', 'If', 'flooding', ',', 'dry', '-', 'type', 'genotype', 'fitness', 'leading', 'offspring', 'therefore', 'dry', 'alleles', 'population', ',', 'however', ',', 'flooded', 'years', 'wet', '-', 'liking', 'phenotype', 'better', 'propagate', 'wet', 'genes', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = [x for x in word_punct_token  if x not in stop_words]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc3c1ac",
      "metadata": {
        "papermill": {
          "duration": 0.016864,
          "end_time": "2022-09-26T11:20:15.775699",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.758835",
          "status": "completed"
        },
        "tags": [],
        "id": "ecc3c1ac"
      },
      "source": [
        "**Lemmatization & stemming**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df06dfd1",
      "metadata": {
        "papermill": {
          "duration": 0.013303,
          "end_time": "2022-09-26T11:20:15.812200",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.798897",
          "status": "completed"
        },
        "tags": [],
        "id": "df06dfd1"
      },
      "source": [
        "> Lemmatizing and stemming both help to reduce the dimension of the vocabulary by return the words to their root form (lemmatizing) or remove all the suffix, affix, prefix and so on (stemming). Stemming is nice for reducing the dimension of vocabulary, but most of the time the word become meaningless as stemming only chopped off the suffix but not returning the words to their base form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "854de752",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:15.840355Z",
          "iopub.status.busy": "2022-09-26T11:20:15.839996Z",
          "iopub.status.idle": "2022-09-26T11:20:16.363372Z",
          "shell.execute_reply": "2022-09-26T11:20:16.362328Z"
        },
        "papermill": {
          "duration": 0.540083,
          "end_time": "2022-09-26T11:20:16.365607",
          "exception": false,
          "start_time": "2022-09-26T11:20:15.825524",
          "status": "completed"
        },
        "tags": [],
        "id": "854de752",
        "outputId": "acc94543-c323-4af7-fa2c-ae184d2eabf6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ddc972c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:16.395136Z",
          "iopub.status.busy": "2022-09-26T11:20:16.394835Z",
          "iopub.status.idle": "2022-09-26T11:20:16.399188Z",
          "shell.execute_reply": "2022-09-26T11:20:16.398318Z"
        },
        "papermill": {
          "duration": 0.021579,
          "end_time": "2022-09-26T11:20:16.401248",
          "exception": false,
          "start_time": "2022-09-26T11:20:16.379669",
          "status": "completed"
        },
        "tags": [],
        "id": "6ddc972c"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c9de28",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:16.431091Z",
          "iopub.status.busy": "2022-09-26T11:20:16.430783Z",
          "iopub.status.idle": "2022-09-26T11:20:18.032735Z",
          "shell.execute_reply": "2022-09-26T11:20:18.031608Z"
        },
        "papermill": {
          "duration": 1.621181,
          "end_time": "2022-09-26T11:20:18.036475",
          "exception": false,
          "start_time": "2022-09-26T11:20:16.415294",
          "status": "completed"
        },
        "tags": [],
        "id": "b8c9de28",
        "outputId": "f558af98-59ef-4005-83c3-d4128cbbbf82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frequency\n"
          ]
        }
      ],
      "source": [
        "print(lemmatizer.lemmatize(tokens[7]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faa62716",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.068918Z",
          "iopub.status.busy": "2022-09-26T11:20:18.068541Z",
          "iopub.status.idle": "2022-09-26T11:20:18.073683Z",
          "shell.execute_reply": "2022-09-26T11:20:18.072689Z"
        },
        "papermill": {
          "duration": 0.023886,
          "end_time": "2022-09-26T11:20:18.075737",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.051851",
          "status": "completed"
        },
        "tags": [],
        "id": "faa62716"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "SBS= SnowballStemmer(language=\"english\")\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "PS = PorterStemmer()\n",
        "\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "LS = LancasterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2996abd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.103762Z",
          "iopub.status.busy": "2022-09-26T11:20:18.103502Z",
          "iopub.status.idle": "2022-09-26T11:20:18.109248Z",
          "shell.execute_reply": "2022-09-26T11:20:18.108352Z"
        },
        "papermill": {
          "duration": 0.022125,
          "end_time": "2022-09-26T11:20:18.111334",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.089209",
          "status": "completed"
        },
        "tags": [],
        "id": "d2996abd",
        "outputId": "3da16b0a-9ffb-4cd6-e48e-92ce0ecbb7a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recognis\n",
            "recognis\n",
            "environ\n"
          ]
        }
      ],
      "source": [
        "print(SBS.stem(\"recognising\"))\n",
        "print(PS.stem(\"recognising\"))\n",
        "print(LS.stem(tokens[20]))       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "038b0c7d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.140167Z",
          "iopub.status.busy": "2022-09-26T11:20:18.139262Z",
          "iopub.status.idle": "2022-09-26T11:20:18.143919Z",
          "shell.execute_reply": "2022-09-26T11:20:18.143083Z"
        },
        "papermill": {
          "duration": 0.021128,
          "end_time": "2022-09-26T11:20:18.145887",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.124759",
          "status": "completed"
        },
        "tags": [],
        "id": "038b0c7d"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "punct = string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00f685a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.174057Z",
          "iopub.status.busy": "2022-09-26T11:20:18.173781Z",
          "iopub.status.idle": "2022-09-26T11:20:18.179255Z",
          "shell.execute_reply": "2022-09-26T11:20:18.178374Z"
        },
        "papermill": {
          "duration": 0.021707,
          "end_time": "2022-09-26T11:20:18.181137",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.159430",
          "status": "completed"
        },
        "tags": [],
        "id": "f00f685a",
        "outputId": "19bbe7a1-6549-4076-8924-19ab5a959b27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "punct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320e666e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.210043Z",
          "iopub.status.busy": "2022-09-26T11:20:18.209177Z",
          "iopub.status.idle": "2022-09-26T11:20:18.215453Z",
          "shell.execute_reply": "2022-09-26T11:20:18.213995Z"
        },
        "papermill": {
          "duration": 0.023285,
          "end_time": "2022-09-26T11:20:18.218113",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.194828",
          "status": "completed"
        },
        "tags": [],
        "id": "320e666e",
        "outputId": "5c07f4f0-182b-4b20-c1ae-97d514bdbd8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A', 'things', 'You', 'might', 'negative', 'frequency', 'dependent', 'selection', 'going', 'least', 'common', 'phenotype', 'reflected', 'genotype', 'going', 'advantage', 'environment', 'For', 'instance', 'prey', 'animal', 'vole', 'light', 'dark', 'phenotype', 'predator', 'might', 'recognize', 'common', 'phenotype', 'food', 'So', 'light', 'voles', 'common', 'foxes', 'may', 'keeping', 'closer', 'eye', 'light', 'phenotypic', 'voles', 'recognising', 'good', 'prey', 'This', 'would', 'reduce', 'light', 'causing', 'alleles', 'due', 'increased', 'predation', 'dark', 'genotypes', 'would', 'increase', 'proportion', 'population', 'scenario', 'reversed', 'This', 'cycle', 'continues', 'perpetually', 'n', 'nHowever', 'unlikely', 'strictly', 'yearly', 'usually', 'takes', 'time', 'year', 'entire', 'populations', 'allele', 'frequencies', 'change', 'enough', 'make', 'large', 'enough', 'difference', 'alter', 'fitness', 'n', 'nMore', 'likely', 'year', 'year', 'basis', 'population', 'experiencing', 'fluctuating', 'selection', 'alternating', 'conditions', 'environment', 'favor', 'one', 'genotype', 'another', 'Perhaps', 'plant', 'species', 'living', 'area', 'flooded', 'every', 'year', 'two', 'phenotypes', 'population', 'plants', 'much', 'better', 'dryer', 'year', 'one', 'better', 'wet', 'year', 'If', 'flooding', 'dry', 'type', 'genotype', 'fitness', 'leading', 'offspring', 'therefore', 'dry', 'alleles', 'population', 'however', 'flooded', 'years', 'wet', 'liking', 'phenotype', 'better', 'propagate', 'wet', 'genes']\n"
          ]
        }
      ],
      "source": [
        "New_tokens = [x for x in tokens if x not in punct]  \n",
        "print(New_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21f9e04d",
      "metadata": {
        "papermill": {
          "duration": 0.013661,
          "end_time": "2022-09-26T11:20:18.246053",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.232392",
          "status": "completed"
        },
        "tags": [],
        "id": "21f9e04d"
      },
      "source": [
        "# Text representation\n",
        "\n",
        "> Feature Extraction is a general term that is also known as a text representation of text vectorization which is a process of converting text into numbers. we call vectorization because when text is converted in numbers it is in vector form."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e60f96",
      "metadata": {
        "papermill": {
          "duration": 0.01356,
          "end_time": "2022-09-26T11:20:18.273357",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.259797",
          "status": "completed"
        },
        "tags": [],
        "id": "60e60f96"
      },
      "source": [
        "# Label encoder\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/386/1*Yp6r7m82IoSnnZDPpDpYNw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d30465",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.302594Z",
          "iopub.status.busy": "2022-09-26T11:20:18.301830Z",
          "iopub.status.idle": "2022-09-26T11:20:18.306489Z",
          "shell.execute_reply": "2022-09-26T11:20:18.305638Z"
        },
        "papermill": {
          "duration": 0.021121,
          "end_time": "2022-09-26T11:20:18.308285",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.287164",
          "status": "completed"
        },
        "tags": [],
        "id": "c9d30465"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da116df0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.337554Z",
          "iopub.status.busy": "2022-09-26T11:20:18.336923Z",
          "iopub.status.idle": "2022-09-26T11:20:18.343912Z",
          "shell.execute_reply": "2022-09-26T11:20:18.342989Z"
        },
        "papermill": {
          "duration": 0.024014,
          "end_time": "2022-09-26T11:20:18.346471",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.322457",
          "status": "completed"
        },
        "tags": [],
        "id": "da116df0",
        "outputId": "6577b28a-b571-4464-855d-6f4b4c255620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 2 0 ... 1 0 0]\n"
          ]
        }
      ],
      "source": [
        "y=le.fit_transform(y)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ae97b48",
      "metadata": {
        "papermill": {
          "duration": 0.013549,
          "end_time": "2022-09-26T11:20:18.374772",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.361223",
          "status": "completed"
        },
        "tags": [],
        "id": "8ae97b48"
      },
      "source": [
        "# Bag Of Words\n",
        "\n",
        "> Bag of words is a little bit similar to one-hot encoding where we enter each word as a binary value and in a Bag of words we keep a single row and entry the count of words in a document.\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/880/1*hLvya7MXjsSc3NS2SoLMEg.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e306d96",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.404142Z",
          "iopub.status.busy": "2022-09-26T11:20:18.403384Z",
          "iopub.status.idle": "2022-09-26T11:20:18.411185Z",
          "shell.execute_reply": "2022-09-26T11:20:18.410198Z"
        },
        "papermill": {
          "duration": 0.024773,
          "end_time": "2022-09-26T11:20:18.413293",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.388520",
          "status": "completed"
        },
        "tags": [],
        "id": "2e306d96",
        "outputId": "a8aa4f4e-bdb1-408a-e03c-03de3e5b607a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       A few things. You might have negative- frequen...\n",
              "1       Is it so hard to believe that there exist part...\n",
              "2                                          There are bees\n",
              "3       I'm a medication technician. And that's alot o...\n",
              "4                          Cesium is such a pretty metal.\n",
              "                              ...                        \n",
              "8690    I make similar observations over the last week...\n",
              "8691                                      You would know.\n",
              "8692              Also use the correct number of sig figs\n",
              "8693    What about the ethical delimmas,  groundbreaki...\n",
              "8694                            I would like to know too.\n",
              "Name: Comment, Length: 8695, dtype: object"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25bcd867",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.442589Z",
          "iopub.status.busy": "2022-09-26T11:20:18.441971Z",
          "iopub.status.idle": "2022-09-26T11:20:18.670340Z",
          "shell.execute_reply": "2022-09-26T11:20:18.669364Z"
        },
        "papermill": {
          "duration": 0.245688,
          "end_time": "2022-09-26T11:20:18.672941",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.427253",
          "status": "completed"
        },
        "tags": [],
        "id": "25bcd867"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "bow = cv.fit_transform(data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c2f427",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.703122Z",
          "iopub.status.busy": "2022-09-26T11:20:18.702817Z",
          "iopub.status.idle": "2022-09-26T11:20:18.708855Z",
          "shell.execute_reply": "2022-09-26T11:20:18.707917Z"
        },
        "papermill": {
          "duration": 0.023607,
          "end_time": "2022-09-26T11:20:18.710841",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.687234",
          "status": "completed"
        },
        "tags": [],
        "id": "17c2f427",
        "outputId": "62955f95-42b0-4f51-b5ae-62f1e5a064b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<8695x18177 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 190076 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b68464",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.740272Z",
          "iopub.status.busy": "2022-09-26T11:20:18.739538Z",
          "iopub.status.idle": "2022-09-26T11:20:18.745866Z",
          "shell.execute_reply": "2022-09-26T11:20:18.744903Z"
        },
        "papermill": {
          "duration": 0.023025,
          "end_time": "2022-09-26T11:20:18.747794",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.724769",
          "status": "completed"
        },
        "tags": [],
        "id": "22b68464",
        "outputId": "f6125c55-19ca-435f-8df0-76c542cd90c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8695, 18177)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f37b4f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:18.778285Z",
          "iopub.status.busy": "2022-09-26T11:20:18.777430Z",
          "iopub.status.idle": "2022-09-26T11:20:19.109075Z",
          "shell.execute_reply": "2022-09-26T11:20:19.108066Z"
        },
        "papermill": {
          "duration": 0.349634,
          "end_time": "2022-09-26T11:20:19.111754",
          "exception": false,
          "start_time": "2022-09-26T11:20:18.762120",
          "status": "completed"
        },
        "tags": [],
        "id": "1f37b4f6"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(max_features=4000)\n",
        "bow = vectorizer.fit_transform(data1).toarray()\n",
        "features = vectorizer.get_feature_names_out()\n",
        "bow = pd.DataFrame(bow, columns=features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b01fdb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:19.142100Z",
          "iopub.status.busy": "2022-09-26T11:20:19.141783Z",
          "iopub.status.idle": "2022-09-26T11:20:19.165476Z",
          "shell.execute_reply": "2022-09-26T11:20:19.164476Z"
        },
        "papermill": {
          "duration": 0.041279,
          "end_time": "2022-09-26T11:20:19.167824",
          "exception": false,
          "start_time": "2022-09-26T11:20:19.126545",
          "status": "completed"
        },
        "tags": [],
        "id": "e8b01fdb",
        "outputId": "19dc1895-36c6-4ce8-b60f-02538ac5cba0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>019</th>\n",
              "      <th>02</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>10th</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>...</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>your</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youtu</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yt</th>\n",
              "      <th>yup</th>\n",
              "      <th>zero</th>\n",
              "      <th>zinc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8693</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8694</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8695 rows × 4000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      000  019  02  10  100  1000  10th  11  12  13  ...  young  younger  \\\n",
              "0       0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "1       0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "2       0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "3       0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "4       0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "...   ...  ...  ..  ..  ...   ...   ...  ..  ..  ..  ...    ...      ...   \n",
              "8690    0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "8691    0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "8692    0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "8693    0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "8694    0    0   0   0    0     0     0   0   0   0  ...      0        0   \n",
              "\n",
              "      your  yourself  youtu  youtube  yt  yup  zero  zinc  \n",
              "0        0         0      0        0   0    0     0     0  \n",
              "1        0         0      0        0   0    0     0     0  \n",
              "2        0         0      0        0   0    0     0     0  \n",
              "3        3         0      0        0   0    0     0     0  \n",
              "4        0         0      0        0   0    0     0     0  \n",
              "...    ...       ...    ...      ...  ..  ...   ...   ...  \n",
              "8690     0         0      0        0   0    0     0     0  \n",
              "8691     0         0      0        0   0    0     0     0  \n",
              "8692     0         0      0        0   0    0     0     0  \n",
              "8693     0         0      0        0   0    0     0     0  \n",
              "8694     0         0      0        0   0    0     0     0  \n",
              "\n",
              "[8695 rows x 4000 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d44b309",
      "metadata": {
        "papermill": {
          "duration": 0.014319,
          "end_time": "2022-09-26T11:20:19.196691",
          "exception": false,
          "start_time": "2022-09-26T11:20:19.182372",
          "status": "completed"
        },
        "tags": [],
        "id": "2d44b309"
      },
      "source": [
        "# N-Grams\n",
        "\n",
        "> The technique is similar to Bag of words. All the techniques till now we have read it is made up of a single word and we are not able to use them or utilize them for better understanding. So N-Gram technique solves this problem and constructs vocabulary with multiple words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![](https://images.deepai.org/glossary-terms/867de904ba9b46869af29cead3194b6c/8ARA1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e17639",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:19.228167Z",
          "iopub.status.busy": "2022-09-26T11:20:19.226671Z",
          "iopub.status.idle": "2022-09-26T11:20:19.834344Z",
          "shell.execute_reply": "2022-09-26T11:20:19.833322Z"
        },
        "papermill": {
          "duration": 0.625808,
          "end_time": "2022-09-26T11:20:19.836915",
          "exception": false,
          "start_time": "2022-09-26T11:20:19.211107",
          "status": "completed"
        },
        "tags": [],
        "id": "38e17639"
      },
      "outputs": [],
      "source": [
        "#Bigram model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "CV = CountVectorizer(ngram_range=[2,2])\n",
        "bow = CV.fit_transform(data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c339139d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:19.868541Z",
          "iopub.status.busy": "2022-09-26T11:20:19.867633Z",
          "iopub.status.idle": "2022-09-26T11:20:19.874403Z",
          "shell.execute_reply": "2022-09-26T11:20:19.873377Z"
        },
        "papermill": {
          "duration": 0.024764,
          "end_time": "2022-09-26T11:20:19.876607",
          "exception": false,
          "start_time": "2022-09-26T11:20:19.851843",
          "status": "completed"
        },
        "tags": [],
        "id": "c339139d",
        "outputId": "1f96e8aa-290b-4ca5-8e3e-46b7b204bd89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<8695x121785 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 228017 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "796d74a8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:19.907752Z",
          "iopub.status.busy": "2022-09-26T11:20:19.906892Z",
          "iopub.status.idle": "2022-09-26T11:20:21.021330Z",
          "shell.execute_reply": "2022-09-26T11:20:21.020327Z"
        },
        "papermill": {
          "duration": 1.132509,
          "end_time": "2022-09-26T11:20:21.023710",
          "exception": false,
          "start_time": "2022-09-26T11:20:19.891201",
          "status": "completed"
        },
        "tags": [],
        "id": "796d74a8"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(ngram_range=[2,2])\n",
        "bow = vectorizer.fit_transform(data1).toarray()\n",
        "features = vectorizer.get_feature_names_out()\n",
        "bow = pd.DataFrame(bow, columns=features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3e1ff6e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:21.056380Z",
          "iopub.status.busy": "2022-09-26T11:20:21.056021Z",
          "iopub.status.idle": "2022-09-26T11:20:21.107380Z",
          "shell.execute_reply": "2022-09-26T11:20:21.106255Z"
        },
        "papermill": {
          "duration": 0.069707,
          "end_time": "2022-09-26T11:20:21.109816",
          "exception": false,
          "start_time": "2022-09-26T11:20:21.040109",
          "status": "completed"
        },
        "tags": [],
        "id": "f3e1ff6e",
        "outputId": "aa1c643b-ffc9-46fc-d88c-443709d84e29"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000 000</th>\n",
              "      <th>000 americans</th>\n",
              "      <th>000 billion</th>\n",
              "      <th>000 btu</th>\n",
              "      <th>000 eggs</th>\n",
              "      <th>000 times</th>\n",
              "      <th>000 without</th>\n",
              "      <th>000 would</th>\n",
              "      <th>000 years</th>\n",
              "      <th>00000000000000000000000332 grams</th>\n",
              "      <th>...</th>\n",
              "      <th>µg scale</th>\n",
              "      <th>µg scales</th>\n",
              "      <th>área so</th>\n",
              "      <th>árvore como</th>\n",
              "      <th>æther or</th>\n",
              "      <th>μg in</th>\n",
              "      <th>μg nnewspapers</th>\n",
              "      <th>μm or</th>\n",
              "      <th>μμ and</th>\n",
              "      <th>الله اکبر</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8693</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8694</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8695 rows × 121785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      000 000  000 americans  000 billion  000 btu  000 eggs  000 times  \\\n",
              "0           0              0            0        0         0          0   \n",
              "1           0              0            0        0         0          0   \n",
              "2           0              0            0        0         0          0   \n",
              "3           0              0            0        0         0          0   \n",
              "4           0              0            0        0         0          0   \n",
              "...       ...            ...          ...      ...       ...        ...   \n",
              "8690        0              0            0        0         0          0   \n",
              "8691        0              0            0        0         0          0   \n",
              "8692        0              0            0        0         0          0   \n",
              "8693        0              0            0        0         0          0   \n",
              "8694        0              0            0        0         0          0   \n",
              "\n",
              "      000 without  000 would  000 years  00000000000000000000000332 grams  \\\n",
              "0               0          0          0                                 0   \n",
              "1               0          0          0                                 0   \n",
              "2               0          0          0                                 0   \n",
              "3               0          0          0                                 0   \n",
              "4               0          0          0                                 0   \n",
              "...           ...        ...        ...                               ...   \n",
              "8690            0          0          0                                 0   \n",
              "8691            0          0          0                                 0   \n",
              "8692            0          0          0                                 0   \n",
              "8693            0          0          0                                 0   \n",
              "8694            0          0          0                                 0   \n",
              "\n",
              "      ...  µg scale  µg scales  área so  árvore como  æther or  μg in  \\\n",
              "0     ...         0          0        0            0         0      0   \n",
              "1     ...         0          0        0            0         0      0   \n",
              "2     ...         0          0        0            0         0      0   \n",
              "3     ...         0          0        0            0         0      0   \n",
              "4     ...         0          0        0            0         0      0   \n",
              "...   ...       ...        ...      ...          ...       ...    ...   \n",
              "8690  ...         0          0        0            0         0      0   \n",
              "8691  ...         0          0        0            0         0      0   \n",
              "8692  ...         0          0        0            0         0      0   \n",
              "8693  ...         0          0        0            0         0      0   \n",
              "8694  ...         0          0        0            0         0      0   \n",
              "\n",
              "      μg nnewspapers  μm or  μμ and  الله اکبر  \n",
              "0                  0      0       0          0  \n",
              "1                  0      0       0          0  \n",
              "2                  0      0       0          0  \n",
              "3                  0      0       0          0  \n",
              "4                  0      0       0          0  \n",
              "...              ...    ...     ...        ...  \n",
              "8690               0      0       0          0  \n",
              "8691               0      0       0          0  \n",
              "8692               0      0       0          0  \n",
              "8693               0      0       0          0  \n",
              "8694               0      0       0          0  \n",
              "\n",
              "[8695 rows x 121785 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08eca7c5",
      "metadata": {
        "papermill": {
          "duration": 0.015016,
          "end_time": "2022-09-26T11:20:21.140409",
          "exception": false,
          "start_time": "2022-09-26T11:20:21.125393",
          "status": "completed"
        },
        "tags": [],
        "id": "08eca7c5"
      },
      "source": [
        "# TF-IDF (Term Frequency and Inverse Document Frequency)\n",
        "\n",
        "> Now the technique which we will study does not work in the same way as the above techniques. This technique gives different values(weightage) to each word in a document. The core idea of assigning weightage is the word that appears multiple time in a document but has a rare appearance in corpus then it is very important for that document so it gives more weightage to that word. This weightage is calculated by two terms known as TF and IDF.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/816/1*1pTLnoOPJKKcKIcRi3q0WA.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0ae602",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:21.172700Z",
          "iopub.status.busy": "2022-09-26T11:20:21.171772Z",
          "iopub.status.idle": "2022-09-26T11:20:21.904588Z",
          "shell.execute_reply": "2022-09-26T11:20:21.903365Z"
        },
        "papermill": {
          "duration": 0.751521,
          "end_time": "2022-09-26T11:20:21.907122",
          "exception": false,
          "start_time": "2022-09-26T11:20:21.155601",
          "status": "completed"
        },
        "tags": [],
        "id": "db0ae602"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_result=tfidf.fit_transform(data1).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760704ed",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:21.939784Z",
          "iopub.status.busy": "2022-09-26T11:20:21.938748Z",
          "iopub.status.idle": "2022-09-26T11:20:21.945923Z",
          "shell.execute_reply": "2022-09-26T11:20:21.944864Z"
        },
        "papermill": {
          "duration": 0.025267,
          "end_time": "2022-09-26T11:20:21.947808",
          "exception": false,
          "start_time": "2022-09-26T11:20:21.922541",
          "status": "completed"
        },
        "tags": [],
        "id": "760704ed",
        "outputId": "2592f271-0410-482a-a3e2-9906f7083475"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a572b0ce",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:21.980058Z",
          "iopub.status.busy": "2022-09-26T11:20:21.979065Z",
          "iopub.status.idle": "2022-09-26T11:20:22.508109Z",
          "shell.execute_reply": "2022-09-26T11:20:22.507112Z"
        },
        "papermill": {
          "duration": 0.547533,
          "end_time": "2022-09-26T11:20:22.510623",
          "exception": false,
          "start_time": "2022-09-26T11:20:21.963090",
          "status": "completed"
        },
        "tags": [],
        "id": "a572b0ce"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=6000)\n",
        "tfidf_result = vectorizer.fit_transform(data1).toarray()\n",
        "features = vectorizer.get_feature_names_out()\n",
        "tfidf_result = pd.DataFrame(tfidf_result, columns=features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36117748",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:22.544782Z",
          "iopub.status.busy": "2022-09-26T11:20:22.543123Z",
          "iopub.status.idle": "2022-09-26T11:20:22.585590Z",
          "shell.execute_reply": "2022-09-26T11:20:22.584581Z"
        },
        "papermill": {
          "duration": 0.061524,
          "end_time": "2022-09-26T11:20:22.587982",
          "exception": false,
          "start_time": "2022-09-26T11:20:22.526458",
          "status": "completed"
        },
        "tags": [],
        "id": "36117748",
        "outputId": "a2ea68c4-fb0e-4128-8973-28902f25e682"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>01</th>\n",
              "      <th>019</th>\n",
              "      <th>02</th>\n",
              "      <th>020</th>\n",
              "      <th>021</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>07</th>\n",
              "      <th>09</th>\n",
              "      <th>...</th>\n",
              "      <th>yours</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youtu</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yt</th>\n",
              "      <th>yup</th>\n",
              "      <th>zeolites</th>\n",
              "      <th>zero</th>\n",
              "      <th>zinc</th>\n",
              "      <th>zp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8693</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8694</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8695 rows × 6000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      000   01  019   02  020  021   03   04   07   09  ...  yours  yourself  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...       ...   \n",
              "8690  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "8691  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "8692  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "8693  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "8694  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0       0.0   \n",
              "\n",
              "      youtu  youtube   yt  yup  zeolites  zero  zinc   zp  \n",
              "0       0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "1       0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "2       0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "3       0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "4       0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "...     ...      ...  ...  ...       ...   ...   ...  ...  \n",
              "8690    0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "8691    0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "8692    0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "8693    0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "8694    0.0      0.0  0.0  0.0       0.0   0.0   0.0  0.0  \n",
              "\n",
              "[8695 rows x 6000 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5828a838",
      "metadata": {
        "papermill": {
          "duration": 0.015451,
          "end_time": "2022-09-26T11:20:22.619125",
          "exception": false,
          "start_time": "2022-09-26T11:20:22.603674",
          "status": "completed"
        },
        "tags": [],
        "id": "5828a838"
      },
      "source": [
        "# Modelling : Naive Bayes\n",
        "\n",
        "\n",
        "> Naive Bayes is a powerful tool that leverages Bayes’ Theorem to understand and mimic complex data structures. In recent years, it has commonly been used for Natural Language Processing (NLP) tasks, such as text categorization.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/600/1*aFhOj7TdBIZir4keHMgHOw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e825ddd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:22.652195Z",
          "iopub.status.busy": "2022-09-26T11:20:22.651419Z",
          "iopub.status.idle": "2022-09-26T11:20:22.658055Z",
          "shell.execute_reply": "2022-09-26T11:20:22.657214Z"
        },
        "papermill": {
          "duration": 0.024914,
          "end_time": "2022-09-26T11:20:22.659951",
          "exception": false,
          "start_time": "2022-09-26T11:20:22.635037",
          "status": "completed"
        },
        "tags": [],
        "id": "5e825ddd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "NB= MultinomialNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29f59b65",
      "metadata": {
        "papermill": {
          "duration": 0.015258,
          "end_time": "2022-09-26T11:20:22.690658",
          "exception": false,
          "start_time": "2022-09-26T11:20:22.675400",
          "status": "completed"
        },
        "tags": [],
        "id": "29f59b65"
      },
      "source": [
        "> **MultinomialNB** implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85686649",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:22.722862Z",
          "iopub.status.busy": "2022-09-26T11:20:22.722598Z",
          "iopub.status.idle": "2022-09-26T11:20:23.072759Z",
          "shell.execute_reply": "2022-09-26T11:20:23.071103Z"
        },
        "papermill": {
          "duration": 0.369945,
          "end_time": "2022-09-26T11:20:23.076393",
          "exception": false,
          "start_time": "2022-09-26T11:20:22.706448",
          "status": "completed"
        },
        "tags": [],
        "id": "85686649",
        "outputId": "5a060126-59b7-4594-defd-9522ba423f06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split(tfidf_result, y, test_size=0.2)\n",
        "NB.fit(xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19b299f7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:23.162737Z",
          "iopub.status.busy": "2022-09-26T11:20:23.162051Z",
          "iopub.status.idle": "2022-09-26T11:20:23.482868Z",
          "shell.execute_reply": "2022-09-26T11:20:23.481484Z"
        },
        "papermill": {
          "duration": 0.369389,
          "end_time": "2022-09-26T11:20:23.487120",
          "exception": false,
          "start_time": "2022-09-26T11:20:23.117731",
          "status": "completed"
        },
        "tags": [],
        "id": "19b299f7"
      },
      "outputs": [],
      "source": [
        "ypred = NB.predict(xtrain)\n",
        "ypred = NB.predict(xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af2ecc55",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:23.570879Z",
          "iopub.status.busy": "2022-09-26T11:20:23.570348Z",
          "iopub.status.idle": "2022-09-26T11:20:23.582808Z",
          "shell.execute_reply": "2022-09-26T11:20:23.581644Z"
        },
        "papermill": {
          "duration": 0.057624,
          "end_time": "2022-09-26T11:20:23.586281",
          "exception": false,
          "start_time": "2022-09-26T11:20:23.528657",
          "status": "completed"
        },
        "tags": [],
        "id": "af2ecc55",
        "outputId": "a7871134-971e-4d1b-baea-7562b442fb10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1739,)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ypred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b10e0e0d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:23.635435Z",
          "iopub.status.busy": "2022-09-26T11:20:23.635154Z",
          "iopub.status.idle": "2022-09-26T11:20:23.640773Z",
          "shell.execute_reply": "2022-09-26T11:20:23.639876Z"
        },
        "papermill": {
          "duration": 0.023777,
          "end_time": "2022-09-26T11:20:23.642554",
          "exception": false,
          "start_time": "2022-09-26T11:20:23.618777",
          "status": "completed"
        },
        "tags": [],
        "id": "b10e0e0d",
        "outputId": "970bc8a9-6d2a-41d6-b889-56369a60f5b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6956,)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ytrain.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1b7cab3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:23.675129Z",
          "iopub.status.busy": "2022-09-26T11:20:23.674873Z",
          "iopub.status.idle": "2022-09-26T11:20:23.678785Z",
          "shell.execute_reply": "2022-09-26T11:20:23.677806Z"
        },
        "papermill": {
          "duration": 0.022596,
          "end_time": "2022-09-26T11:20:23.680736",
          "exception": false,
          "start_time": "2022-09-26T11:20:23.658140",
          "status": "completed"
        },
        "tags": [],
        "id": "e1b7cab3"
      },
      "outputs": [],
      "source": [
        "ytrain=ytrain[0:1739]\n",
        "ytest=ytest[0:1739]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bdcbb33",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-26T11:20:23.713884Z",
          "iopub.status.busy": "2022-09-26T11:20:23.713182Z",
          "iopub.status.idle": "2022-09-26T11:20:23.729160Z",
          "shell.execute_reply": "2022-09-26T11:20:23.727539Z"
        },
        "papermill": {
          "duration": 0.035311,
          "end_time": "2022-09-26T11:20:23.731641",
          "exception": false,
          "start_time": "2022-09-26T11:20:23.696330",
          "status": "completed"
        },
        "tags": [],
        "id": "9bdcbb33",
        "outputId": "c8f4ed6f-6745-49c8-af2c-dd489ed9aa1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Results:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.56      0.47       706\n",
            "           1       0.35      0.30      0.32       594\n",
            "           2       0.24      0.13      0.17       439\n",
            "\n",
            "    accuracy                           0.36      1739\n",
            "   macro avg       0.33      0.33      0.32      1739\n",
            "weighted avg       0.34      0.36      0.34      1739\n",
            "\n",
            "\n",
            "Testing Results:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.87      0.75       748\n",
            "           1       0.68      0.63      0.66       560\n",
            "           2       0.88      0.48      0.62       431\n",
            "\n",
            "    accuracy                           0.70      1739\n",
            "   macro avg       0.74      0.66      0.68      1739\n",
            "weighted avg       0.72      0.70      0.69      1739\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Results:\\n\")\n",
        "print(classification_report(ytrain, ypred))\n",
        "print(\"\\nTesting Results:\\n\")\n",
        "print(classification_report(ytest, ypred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 19.294285,
      "end_time": "2022-09-26T11:20:24.668087",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-09-26T11:20:05.373802",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}